{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfc2a5d8-091b-46e9-81b0-4a9382727c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anonymous/miniforge3/lib/python3.9/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import json\n",
    "from transformers import AutoTokenizer, BertModel\n",
    "import tqdm\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "# custom dataset class to load our data\n",
    "\n",
    "class OurDataset(Dataset):\n",
    "    def __init__(self, data_file, labels_file):\n",
    "        self.full_data = json.load(open(data_file))\n",
    "        self.labels = torch.load(labels_file)\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "        self.model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.full_data) \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        inputs = self.tokenizer(self.full_data[idx], return_tensors=\"pt\")\n",
    "        outputs = self.model(**inputs)\n",
    "        last_hidden_states = outputs.last_hidden_state\n",
    "        return last_hidden_states, self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e875bcf5-14bd-4b55-a263-7febb0003dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import openai\n",
    "openai.api_key = \"\"\n",
    "\n",
    "class EvoPrompting:\n",
    "    def __init__(self, T, M, K, seed_folder, prepend_file, param_threshold):\n",
    "        self.T = T  #how many total evolution steps to go through\n",
    "        self.M = M  #how many children to generate in each evolution step\n",
    "        self.K = K  #how many children are allowed to survive in each evolution step\n",
    "        self.seed_folder = seed_folder\n",
    "        self.prepend_file = prepend_file\n",
    "        self.param_threshold = param_threshold\n",
    "        self.population = []\n",
    "        self.ds = OurDataset(\"data/full_data.json\", \"data/labels.torch\")\n",
    "        self.curr_time = datetime.now()\n",
    "        self.initialize_population()\n",
    "        \n",
    "    def initialize_population(self):\n",
    "        def read_seed_files(file_path):\n",
    "            with open(file_path, \"r\") as file:\n",
    "                return file.read()\n",
    "        \n",
    "        self.prepend_code = read_seed_files(self.prepend_file)\n",
    "        seed_files = [f for f in os.listdir(self.seed_folder) if f.endswith('.py')]\n",
    "        for seed_file in seed_files:\n",
    "            seed_file_path = os.path.join(self.seed_folder, seed_file)\n",
    "            seed_code = read_seed_files(seed_file_path)\n",
    "            model, metrics = self.exec_code(seed_code)\n",
    "            if (model, metrics) == (0, 0):\n",
    "                continue\n",
    "            self.population.append((seed_code, model, metrics))\n",
    "        \n",
    "    def exec_code(self, code):\n",
    "        def single_evaluation():\n",
    "            print(\"Executing code segment\")\n",
    "            exec(self.prepend_code, globals())\n",
    "            exec(code, globals())\n",
    "            model = globals()['main']()\n",
    "            return model\n",
    "        def count_parameters(model):\n",
    "            return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        \n",
    "        model = single_evaluation()\n",
    "        if count_parameters(model) >= self.param_threshold:\n",
    "            return (0, 0)\n",
    "        metrics = self.eval_model(model)\n",
    "        print(\"No. of Parameters: \", count_parameters(model))\n",
    "        print(\"Model Score: \", metrics)\n",
    "        return model, metrics\n",
    "    \n",
    "    #evaluate model without training here (just a classification forward pass)\n",
    "    def eval_model(self, model):\n",
    "        loss_fn = torch.nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=0.00001, momentum=0.9)\n",
    "        loss = None\n",
    "        for i in range(50):\n",
    "            idx = random.randint(0, self.ds.__len__()-3000)\n",
    "            x, y = self.ds.__getitem__(idx)\n",
    "            y = y.long()\n",
    "            outputs = model(x)\n",
    "            loss = loss_fn(outputs, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        correct = 0\n",
    "        for i in range(50):\n",
    "            idx = random.randint(0, self.ds.__len__()-3000)\n",
    "            x, y = self.ds.__getitem__(idx)\n",
    "            y = y.long()\n",
    "            outputs = model(x)\n",
    "            outputs = torch.argmax(outputs, axis=-1)\n",
    "            if outputs == y:\n",
    "                correct += 1\n",
    "        return correct/50\n",
    "    \n",
    "    def evaluate_children(self, child_architectures):\n",
    "        for code in child_architectures:\n",
    "            try:\n",
    "                model, metrics = self.exec_code(code)\n",
    "                self.population.append((code, model, metrics))\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    # setup prompt and generate children\n",
    "    def cross_mutation(self):\n",
    "        while (datetime.now() - self.curr_time).seconds <= 60:\n",
    "            time.sleep(2)\n",
    "        self.curr_time = datetime.now()\n",
    "        child_architectures = []\n",
    "        for m in range(self.M):\n",
    "            prompt = \"\"\n",
    "            for arch in self.population: \n",
    "                prompt += \"Code: \" + arch[0] + \"\\n\\n\"\n",
    "            prompt += \"Code:\"\n",
    "            #print(prompt)\n",
    "            messages = [#{\"role\":\"system\", \"content\":\"You are a helpful assistant whose job is to generate better models based on the example given to you. Only generate valid python code.\"},\n",
    "                       {\"role\":\"user\", \"content\":prompt}]\n",
    "            response = openai.Completion.create(model=\"gpt-3.5-turbo-instruct\", prompt=prompt, n=1, max_tokens=1600, temperature=0)\n",
    "            #response = client.chat.completions.create(model=\"gpt-3.5-turbo-instruct\", messages=messages, n=1, max_tokens=1600, temperature=0)\n",
    "            # print(response[\"choices\"][0][\"text\"].lstrip())\n",
    "            child_architectures.append(response[\"choices\"][0][\"text\"].lstrip())\n",
    "        return child_architectures\n",
    "    \n",
    "    def cull_population(self):\n",
    "        self.population = sorted(self.population, key=lambda x: x[-1], reverse=True)[:self.K]\n",
    "        \n",
    "    def evolve(self):\n",
    "        for t in range(self.T):\n",
    "            child_architectures = self.cross_mutation()\n",
    "            self.evaluate_children(child_architectures)\n",
    "            self.cull_population()\n",
    "            print(\"Current Population Scores: \", [p[-1] for p in self.population])\n",
    "        return self.population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a64b28e4-7e24-43f1-a322-359fc4055ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing code segment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anonymous/miniforge3/lib/python3.9/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of Parameters:  12614915\n",
      "Model Score:  0.38\n",
      "Executing code segment\n",
      "No. of Parameters:  6112259\n",
      "Model Score:  0.36\n",
      "Executing code segment\n",
      "No. of Parameters:  11041027\n",
      "Model Score:  0.42\n",
      "Executing code segment\n",
      "No. of Parameters:  11631619\n",
      "Model Score:  0.4\n",
      "Executing code segment\n",
      "No. of Parameters:  11631619\n",
      "Model Score:  0.32\n",
      "Executing code segment\n",
      "No. of Parameters:  6112259\n",
      "Model Score:  0.4\n",
      "Executing code segment\n",
      "No. of Parameters:  12614915\n",
      "Model Score:  0.42\n"
     ]
    }
   ],
   "source": [
    "evo = EvoPrompting(T=4, M=3, K=3, seed_folder=\"./seeds\", prepend_file=\"./prepend.py\", param_threshold=20000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d49e53b5-3668-4c95-b51c-2e0179d971ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing code segment\n",
      "Executing code segment\n",
      "Executing code segment\n",
      "Current Population Scores:  [0.42, 0.42, 0.4]\n",
      "Executing code segment\n",
      "No. of Parameters:  12222211\n",
      "Model Score:  0.34\n",
      "Executing code segment\n",
      "No. of Parameters:  12222211\n",
      "Model Score:  0.4\n",
      "Executing code segment\n",
      "No. of Parameters:  12222211\n",
      "Model Score:  0.42\n",
      "Current Population Scores:  [0.42, 0.42, 0.42]\n",
      "Executing code segment\n",
      "No. of Parameters:  12222211\n",
      "Model Score:  0.44\n",
      "Executing code segment\n",
      "No. of Parameters:  12222211\n",
      "Model Score:  0.46\n",
      "Executing code segment\n",
      "No. of Parameters:  12222211\n",
      "Model Score:  0.44\n",
      "Current Population Scores:  [0.46, 0.44, 0.44]\n",
      "Executing code segment\n",
      "No. of Parameters:  12222211\n",
      "Model Score:  0.38\n",
      "Executing code segment\n",
      "No. of Parameters:  12222211\n",
      "Model Score:  0.32\n",
      "Executing code segment\n",
      "No. of Parameters:  12222211\n",
      "Model Score:  0.58\n",
      "Current Population Scores:  [0.58, 0.46, 0.44]\n"
     ]
    }
   ],
   "source": [
    "population = evo.evolve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "812d6dbb-6c48-48df-a4c5-6bc5610a3cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class TransformerClassifier(nn.Module):\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        d_model=768,\n",
      "        n_classes=3,\n",
      "        nhead=4,\n",
      "        dim_feedforward=256,\n",
      "        num_layers=4,\n",
      "        dropout=0.1,\n",
      "        activation=\"relu\",\n",
      "        classifier_dropout=0.1,\n",
      "    ):\n",
      "\n",
      "        super().__init__()\n",
      "\n",
      "        self.pos_encoder = PositionalEncoding(\n",
      "            d_model=d_model,\n",
      "            dropout=dropout,\n",
      "            max_len=5000,\n",
      "        )\n",
      "\n",
      "        encoder_layer = nn.TransformerEncoderLayer(\n",
      "            d_model=d_model,\n",
      "            nhead=nhead,\n",
      "            dim_feedforward=dim_feedforward,\n",
      "            dropout=dropout,\n",
      "        )\n",
      "        self.transformer_encoder = nn.TransformerEncoder(\n",
      "            encoder_layer,\n",
      "            num_layers=num_layers,\n",
      "        )\n",
      "        self.classifier1 = nn.Linear(d_model, d_model)\n",
      "        self.classifier2 = nn.Linear(d_model, d_model)\n",
      "        self.classifier3 = nn.Linear(d_model, n_classes)\n",
      "        self.softmax = nn.Softmax(dim=1)\n",
      "        self.d_model = d_model\n",
      "\n",
      "    def forward(self, x):\n",
      "        x = self.pos_encoder(x)\n",
      "        x = self.transformer_encoder(x)\n",
      "        x = x.mean(dim=1)\n",
      "        x = self.classifier1(x)\n",
      "        x = self.classifier2(x)\n",
      "        x = self.classifier3(x)\n",
      "        \n",
      "        return self.softmax(x)\n",
      "class TransformerClassifier(nn.Module):\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        d_model=768,\n",
      "        n_classes=3,\n",
      "        nhead=4,\n",
      "        dim_feedforward=256,\n",
      "        num_layers=4,\n",
      "        dropout=0.1,\n",
      "        activation=\"relu\",\n",
      "        classifier_dropout=0.1,\n",
      "    ):\n",
      "\n",
      "        super().__init__()\n",
      "\n",
      "        self.pos_encoder = PositionalEncoding(\n",
      "            d_model=d_model,\n",
      "            dropout=dropout,\n",
      "            max_len=5000,\n",
      "        )\n",
      "\n",
      "        encoder_layer = nn.TransformerEncoderLayer(\n",
      "            d_model=d_model,\n",
      "            nhead=nhead,\n",
      "            dim_feedforward=dim_feedforward,\n",
      "            dropout=dropout,\n",
      "        )\n",
      "        self.transformer_encoder = nn.TransformerEncoder(\n",
      "            encoder_layer,\n",
      "            num_layers=num_layers,\n",
      "        )\n",
      "        self.classifier1 = nn.Linear(d_model, d_model)\n",
      "        self.classifier2 = nn.Linear(d_model, d_model)\n",
      "        self.classifier3 = nn.Linear(d_model, n_classes)\n",
      "        self.softmax = nn.Softmax(dim=1)\n",
      "        self.d_model = d_model\n",
      "\n",
      "    def forward(self, x):\n",
      "        x = self.pos_encoder(x)\n",
      "        x = self.transformer_encoder(x)\n",
      "        x = x.mean(dim=1)\n",
      "        x = self.classifier1(x)\n",
      "        x = self.classifier2(x)\n",
      "        x = self.classifier3(x)\n",
      "        \n",
      "        return self.softmax(x)\n",
      "class TransformerClassifier(nn.Module):\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        d_model=768,\n",
      "        n_classes=3,\n",
      "        nhead=4,\n",
      "        dim_feedforward=256,\n",
      "        num_layers=4,\n",
      "        dropout=0.1,\n",
      "        activation=\"relu\",\n",
      "        classifier_dropout=0.1,\n",
      "    ):\n",
      "\n",
      "        super().__init__()\n",
      "\n",
      "        self.pos_encoder = PositionalEncoding(\n",
      "            d_model=d_model,\n",
      "            dropout=dropout,\n",
      "            max_len=5000,\n",
      "        )\n",
      "\n",
      "        encoder_layer = nn.TransformerEncoderLayer(\n",
      "            d_model=d_model,\n",
      "            nhead=nhead,\n",
      "            dim_feedforward=dim_feedforward,\n",
      "            dropout=dropout,\n",
      "        )\n",
      "        self.transformer_encoder = nn.TransformerEncoder(\n",
      "            encoder_layer,\n",
      "            num_layers=num_layers,\n",
      "        )\n",
      "        self.classifier1 = nn.Linear(d_model, d_model)\n",
      "        self.classifier2 = nn.Linear(d_model, d_model)\n",
      "        self.classifier3 = nn.Linear(d_model, n_classes)\n",
      "        self.softmax = nn.Softmax(dim=1)\n",
      "        self.d_model = d_model\n",
      "\n",
      "    def forward(self, x):\n",
      "        x = self.pos_encoder(x)\n",
      "        x = self.transformer_encoder(x)\n",
      "        x = x.mean(dim=1)\n",
      "        x = self.classifier1(x)\n",
      "        x = self.classifier2(x)\n",
      "        x = self.classifier3(x)\n",
      "        \n",
      "        return self.softmax(x)\n"
     ]
    }
   ],
   "source": [
    "for p in population:\n",
    "    print(p[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "62f74fbf-f545-4ab5-ba80-7606b6dfbfb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tests.genbig import TransformerClassifier\n",
    "t = TransformerClassifier()\n",
    "ds = OurDataset(\"data/full_data.json\", \"data/labels.torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "bbf78a12-2da3-4cac-ac21-b6f1979d1e29",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.9759292602539062\n",
      "10 1.1690107583999634\n",
      "20 1.1952286958694458\n",
      "30 1.0678808689117432\n",
      "40 1.2093340158462524\n",
      "50 1.2363804578781128\n",
      "60 0.7992780804634094\n",
      "70 1.4137789011001587\n",
      "80 0.6471167802810669\n",
      "90 0.6237916946411133\n",
      "100 0.5968904495239258\n",
      "110 0.5764997601509094\n",
      "120 0.5660012364387512\n",
      "130 1.5397109985351562\n",
      "140 0.5550039410591125\n",
      "150 0.5527303814888\n",
      "160 0.5520373582839966\n",
      "170 0.5517510771751404\n",
      "180 0.5515452027320862\n",
      "190 0.5515131950378418\n",
      "200 0.551461935043335\n",
      "210 0.5514549016952515\n",
      "220 0.5514509677886963\n",
      "230 1.5514402389526367\n",
      "240 0.5514451861381531\n",
      "250 0.5514450669288635\n",
      "260 0.5514447689056396\n",
      "270 0.5514447689056396\n",
      "280 0.5514447093009949\n",
      "290 1.5514447689056396\n",
      "300 0.5514447093009949\n",
      "310 0.5514447093009949\n",
      "320 0.5514447093009949\n",
      "330 1.5514447689056396\n",
      "340 0.5514447093009949\n",
      "350 0.5514447093009949\n",
      "360 1.5514447689056396\n",
      "370 1.5514447689056396\n",
      "380 1.5514447689056396\n",
      "390 0.5514447093009949\n",
      "400 1.5514447689056396\n",
      "410 0.5514447093009949\n",
      "420 1.5514447689056396\n",
      "430 1.5514447689056396\n",
      "440 0.5514447093009949\n",
      "450 0.5514447093009949\n",
      "460 1.5514447689056396\n",
      "470 1.5514447689056396\n",
      "480 1.5514447689056396\n",
      "490 0.5514447093009949\n",
      "500 0.5514447093009949\n",
      "510 0.5514447093009949\n",
      "520 0.5514447093009949\n",
      "530 0.5514447093009949\n",
      "540 0.5514447093009949\n",
      "550 0.5514447093009949\n",
      "560 0.5514447093009949\n",
      "570 1.5514447689056396\n",
      "580 1.5514447689056396\n",
      "590 0.5514447093009949\n",
      "600 1.5514447689056396\n",
      "610 0.5514447093009949\n",
      "620 0.5514447093009949\n",
      "630 1.5514447689056396\n",
      "640 0.5514447093009949\n",
      "650 1.5514447689056396\n",
      "660 1.5514447689056396\n",
      "670 0.5514447093009949\n",
      "680 0.5514447093009949\n",
      "690 0.5514447093009949\n",
      "700 0.5514447093009949\n",
      "710 1.5514447689056396\n",
      "720 1.5514447689056396\n",
      "730 1.5514447689056396\n",
      "740 0.5514447093009949\n",
      "750 1.5514447689056396\n",
      "760 0.5514447093009949\n",
      "770 1.5514447689056396\n",
      "780 0.5514447093009949\n",
      "790 1.5514447689056396\n",
      "800 0.5514447093009949\n",
      "810 1.5514447689056396\n",
      "820 0.5514447093009949\n",
      "830 0.5514447093009949\n",
      "840 1.5514447689056396\n",
      "850 0.5514447093009949\n",
      "860 0.5514447093009949\n",
      "870 1.5514447689056396\n",
      "880 0.5514447093009949\n",
      "890 0.5514447093009949\n",
      "900 1.5514447689056396\n",
      "910 1.5514447689056396\n",
      "920 0.5514447093009949\n",
      "930 0.5514447093009949\n",
      "940 1.5514447689056396\n",
      "950 0.5514447093009949\n",
      "960 0.5514447093009949\n",
      "970 1.5514447689056396\n",
      "980 0.5514447093009949\n",
      "990 1.5514447689056396\n",
      "1000 0.5514447093009949\n",
      "1010 0.5514447093009949\n",
      "1020 0.5514447093009949\n",
      "1030 0.5514447093009949\n",
      "1040 0.5514447093009949\n",
      "1050 0.5514447093009949\n",
      "1060 0.5514447093009949\n",
      "1070 0.5514447093009949\n",
      "1080 1.5514447689056396\n",
      "1090 0.5514447093009949\n",
      "1100 0.5514447093009949\n",
      "1110 1.5514447689056396\n",
      "1120 0.5514447093009949\n",
      "1130 0.5514447093009949\n",
      "1140 1.5514447689056396\n",
      "1150 1.5514447689056396\n",
      "1160 1.5514447689056396\n",
      "1170 0.5514447093009949\n",
      "1180 0.5514447093009949\n",
      "1190 0.5514447093009949\n",
      "1200 0.5514447093009949\n",
      "1210 0.5514447093009949\n",
      "1220 0.5514447093009949\n",
      "1230 0.5514447093009949\n",
      "1240 0.5514447093009949\n",
      "1250 0.5514447093009949\n",
      "1260 0.5514447093009949\n",
      "1270 1.5514447689056396\n",
      "1280 1.5514447689056396\n",
      "1290 1.5514447689056396\n",
      "1300 0.5514447093009949\n",
      "1310 0.5514447093009949\n",
      "1320 0.5514447093009949\n",
      "1330 0.5514447093009949\n",
      "1340 0.5514447093009949\n",
      "1350 0.5514447093009949\n",
      "1360 0.5514447093009949\n",
      "1370 0.5514447093009949\n",
      "1380 1.5514447689056396\n",
      "1390 1.5514447689056396\n",
      "1400 0.5514447093009949\n",
      "1410 1.5514447689056396\n",
      "1420 0.5514447093009949\n",
      "1430 1.5514447689056396\n",
      "1440 1.5514447689056396\n",
      "1450 0.5514447093009949\n",
      "1460 0.5514447093009949\n",
      "1470 0.5514447093009949\n",
      "1480 0.5514447093009949\n",
      "1490 1.5514447689056396\n",
      "1500 1.5514447689056396\n",
      "1510 1.5514447689056396\n",
      "1520 1.5514447689056396\n",
      "1530 1.5514447689056396\n",
      "1540 0.5514447093009949\n",
      "1550 0.5514447093009949\n",
      "1560 0.5514447093009949\n",
      "1570 0.5514447093009949\n",
      "1580 0.5514447093009949\n",
      "1590 0.5514447093009949\n",
      "1600 0.5514447093009949\n",
      "1610 1.5514447689056396\n",
      "1620 1.5514447689056396\n",
      "1630 1.5514447689056396\n",
      "1640 1.5514447689056396\n",
      "1650 0.5514447093009949\n",
      "1660 1.5514447689056396\n",
      "1670 0.5514447093009949\n",
      "1680 0.5514447093009949\n",
      "1690 0.5514447093009949\n",
      "1700 1.5514447689056396\n",
      "1710 1.5514447689056396\n",
      "1720 0.5514447093009949\n",
      "1730 0.5514447093009949\n",
      "1740 0.5514447093009949\n",
      "1750 1.5514447689056396\n",
      "1760 0.5514447093009949\n",
      "1770 0.5514447093009949\n",
      "1780 1.5514447689056396\n",
      "1790 1.5514447689056396\n",
      "1800 0.5514447093009949\n",
      "1810 0.5514447093009949\n",
      "1820 0.5514447093009949\n",
      "1830 1.5514447689056396\n",
      "1840 1.5514447689056396\n",
      "1850 0.5514447093009949\n",
      "1860 1.5514447689056396\n",
      "1870 1.5514447689056396\n",
      "1880 0.5514447093009949\n",
      "1890 0.5514447093009949\n",
      "1900 0.5514447093009949\n",
      "1910 0.5514447093009949\n",
      "1920 0.5514447093009949\n",
      "1930 0.5514447093009949\n",
      "1940 1.5514447689056396\n",
      "1950 0.5514447093009949\n",
      "1960 1.5514447689056396\n",
      "1970 1.5514447689056396\n",
      "1980 1.5514447689056396\n",
      "1990 0.5514447093009949\n",
      "2000 0.5514447093009949\n",
      "2010 1.5514447689056396\n",
      "2020 0.5514447093009949\n",
      "2030 0.5514447093009949\n",
      "2040 1.5514447689056396\n",
      "2050 0.5514447093009949\n",
      "2060 0.5514447093009949\n",
      "2070 0.5514447093009949\n",
      "2080 0.5514447093009949\n",
      "2090 0.5514447093009949\n",
      "2100 1.5514447689056396\n",
      "2110 0.5514447093009949\n",
      "2120 0.5514447093009949\n",
      "2130 0.5514447093009949\n",
      "2140 0.5514447093009949\n",
      "2150 0.5514447093009949\n",
      "2160 0.5514447093009949\n",
      "2170 1.5514447689056396\n",
      "2180 1.5514447689056396\n",
      "2190 1.5514447689056396\n",
      "2200 0.5514447093009949\n",
      "2210 0.5514447093009949\n",
      "2220 0.5514447093009949\n",
      "2230 0.5514447093009949\n",
      "2240 0.5514447093009949\n",
      "2250 0.5514447093009949\n",
      "2260 0.5514447093009949\n",
      "2270 0.5514447093009949\n",
      "2280 0.5514447093009949\n",
      "2290 1.5514447689056396\n",
      "2300 0.5514447093009949\n",
      "2310 0.5514447093009949\n",
      "2320 1.5514447689056396\n",
      "2330 0.5514447093009949\n",
      "2340 0.5514447093009949\n",
      "2350 1.5514447689056396\n",
      "2360 0.5514447093009949\n",
      "2370 0.5514447093009949\n",
      "2380 0.5514447093009949\n",
      "2390 0.5514447093009949\n",
      "2400 0.5514447093009949\n",
      "2410 0.5514447093009949\n",
      "2420 0.5514447093009949\n",
      "2430 0.5514447093009949\n",
      "2440 0.5514447093009949\n",
      "2450 0.5514447093009949\n",
      "2460 1.5514447689056396\n",
      "2470 0.5514447093009949\n",
      "2480 0.5514447093009949\n",
      "2490 0.5514447093009949\n",
      "2500 0.5514447093009949\n",
      "2510 1.5514447689056396\n",
      "2520 1.5514447689056396\n",
      "2530 0.5514447093009949\n",
      "2540 0.5514447093009949\n",
      "2550 1.5514447689056396\n",
      "2560 1.5514447689056396\n",
      "2570 0.5514447093009949\n",
      "2580 0.5514447093009949\n",
      "2590 0.5514447093009949\n",
      "2600 0.5514447093009949\n",
      "2610 0.5514447093009949\n",
      "2620 1.5514447689056396\n",
      "2630 1.5514447689056396\n",
      "2640 1.5514447689056396\n",
      "2650 1.5514447689056396\n",
      "2660 1.5514447689056396\n",
      "2670 0.5514447093009949\n",
      "2680 1.5514447689056396\n",
      "2690 0.5514447093009949\n",
      "2700 1.5514447689056396\n",
      "2710 0.5514447093009949\n",
      "2720 1.5514447689056396\n",
      "2730 0.5514447093009949\n",
      "2740 0.5514447093009949\n",
      "2750 0.5514447093009949\n",
      "2760 0.5514447093009949\n",
      "2770 1.5514447689056396\n",
      "2780 1.5514447689056396\n",
      "2790 1.5514447689056396\n",
      "2800 1.5514447689056396\n",
      "2810 0.5514447093009949\n",
      "2820 0.5514447093009949\n",
      "2830 0.5514447093009949\n",
      "2840 1.5514447689056396\n",
      "2850 0.5514447093009949\n",
      "2860 0.5514447093009949\n",
      "2870 0.5514447093009949\n",
      "2880 0.5514447093009949\n",
      "2890 0.5514447093009949\n",
      "2900 0.5514447093009949\n",
      "2910 1.5514447689056396\n",
      "2920 0.5514447093009949\n",
      "2930 0.5514447093009949\n",
      "2940 0.5514447093009949\n",
      "2950 0.5514447093009949\n",
      "2960 0.5514447093009949\n",
      "2970 0.5514447093009949\n",
      "2980 0.5514447093009949\n",
      "2990 0.5514447093009949\n",
      "3000 0.5514447093009949\n",
      "3010 0.5514447093009949\n",
      "3020 0.5514447093009949\n",
      "3030 1.5514447689056396\n",
      "3040 1.5514447689056396\n",
      "3050 0.5514447093009949\n",
      "3060 0.5514447093009949\n",
      "3070 0.5514447093009949\n",
      "3080 0.5514447093009949\n",
      "3090 0.5514447093009949\n",
      "3100 0.5514447093009949\n",
      "3110 0.5514447093009949\n",
      "3120 1.5514447689056396\n",
      "3130 0.5514447093009949\n",
      "3140 0.5514447093009949\n",
      "3150 0.5514447093009949\n",
      "3160 0.5514447093009949\n",
      "3170 1.5514447689056396\n",
      "3180 0.5514447093009949\n",
      "3190 0.5514447093009949\n",
      "3200 0.5514447093009949\n",
      "3210 1.5514447689056396\n",
      "3220 0.5514447093009949\n",
      "3230 0.5514447093009949\n",
      "3240 1.5514447689056396\n",
      "3250 0.5514447093009949\n",
      "3260 1.5514447689056396\n",
      "3270 0.5514447093009949\n",
      "3280 1.5514447689056396\n",
      "3290 0.5514447093009949\n",
      "3300 1.5514447689056396\n",
      "3310 0.5514447093009949\n",
      "3320 0.5514447093009949\n",
      "3330 0.5514447093009949\n",
      "3340 0.5514447093009949\n",
      "3350 1.5514447689056396\n",
      "3360 0.5514447093009949\n",
      "3370 0.5514447093009949\n",
      "3380 1.5514447689056396\n",
      "3390 0.5514447093009949\n",
      "3400 0.5514447093009949\n",
      "3410 0.5514447093009949\n",
      "3420 1.5514447689056396\n",
      "3430 0.5514447093009949\n",
      "3440 1.5514447689056396\n",
      "3450 0.5514447093009949\n",
      "3460 0.5514447093009949\n",
      "3470 0.5514447093009949\n",
      "3480 1.5514447689056396\n",
      "3490 0.5514447093009949\n",
      "3500 1.5514447689056396\n",
      "3510 0.5514447093009949\n",
      "3520 0.5514447093009949\n",
      "3530 0.5514447093009949\n",
      "3540 1.5514447689056396\n",
      "3550 0.5514447093009949\n",
      "3560 0.5514447093009949\n",
      "3570 0.5514447093009949\n",
      "3580 1.5514447689056396\n",
      "3590 0.5514447093009949\n",
      "3600 0.5514447093009949\n",
      "3610 1.5514447689056396\n",
      "3620 0.5514447093009949\n",
      "3630 0.5514447093009949\n",
      "3640 1.5514447689056396\n",
      "3650 0.5514447093009949\n",
      "3660 0.5514447093009949\n",
      "3670 0.5514447093009949\n",
      "3680 0.5514447093009949\n",
      "3690 0.5514447093009949\n",
      "3700 1.5514447689056396\n",
      "3710 1.5514447689056396\n",
      "3720 1.5514447689056396\n",
      "3730 0.5514447093009949\n",
      "3740 0.5514447093009949\n",
      "3750 0.5514447093009949\n",
      "3760 0.5514447093009949\n",
      "3770 0.5514447093009949\n",
      "3780 0.5514447093009949\n",
      "3790 1.5514447689056396\n",
      "3800 0.5514447093009949\n",
      "3810 0.5514447093009949\n",
      "3820 0.5514447093009949\n",
      "3830 1.5514447689056396\n",
      "3840 1.5514447689056396\n",
      "3850 1.5514447689056396\n",
      "3860 0.5514447093009949\n",
      "3870 1.5514447689056396\n",
      "3880 0.5514447093009949\n",
      "3890 0.5514447093009949\n",
      "3900 0.5514447093009949\n",
      "3910 0.5514447093009949\n",
      "3920 0.5514447093009949\n",
      "3930 0.5514447093009949\n",
      "3940 1.5514447689056396\n",
      "3950 1.5514447689056396\n",
      "3960 1.5514447689056396\n",
      "3970 0.5514447093009949\n",
      "3980 0.5514447093009949\n",
      "3990 1.5514447689056396\n",
      "4000 1.5514447689056396\n",
      "4010 1.5514447689056396\n",
      "4020 1.5514447689056396\n",
      "4030 1.5514447689056396\n",
      "4040 0.5514447093009949\n",
      "4050 0.5514447093009949\n",
      "4060 0.5514447093009949\n",
      "4070 0.5514447093009949\n",
      "4080 0.5514447093009949\n",
      "4090 1.5514447689056396\n",
      "4100 0.5514447093009949\n",
      "4110 1.5514447689056396\n",
      "4120 0.5514447093009949\n",
      "4130 0.5514447093009949\n",
      "4140 0.5514447093009949\n",
      "4150 1.5514447689056396\n",
      "4160 0.5514447093009949\n",
      "4170 1.5514447689056396\n",
      "4180 0.5514447093009949\n",
      "4190 0.5514447093009949\n",
      "4200 0.5514447093009949\n",
      "4210 0.5514447093009949\n",
      "4220 0.5514447093009949\n",
      "4230 0.5514447093009949\n",
      "4240 0.5514447093009949\n",
      "4250 0.5514447093009949\n",
      "4260 0.5514447093009949\n",
      "4270 1.5514447689056396\n",
      "4280 0.5514447093009949\n",
      "4290 1.5514447689056396\n",
      "4300 1.5514447689056396\n",
      "4310 0.5514447093009949\n",
      "4320 0.5514447093009949\n",
      "4330 0.5514447093009949\n",
      "4340 0.5514447093009949\n",
      "4350 1.5514447689056396\n",
      "4360 0.5514447093009949\n",
      "4370 0.5514447093009949\n",
      "4380 0.5514447093009949\n",
      "4390 1.5514447689056396\n",
      "4400 0.5514447093009949\n",
      "4410 0.5514447093009949\n",
      "4420 1.5514447689056396\n",
      "4430 0.5514447093009949\n",
      "4440 1.5514447689056396\n",
      "4450 0.5514447093009949\n",
      "4460 0.5514447093009949\n",
      "4470 0.5514447093009949\n",
      "4480 0.5514447093009949\n",
      "4490 0.5514447093009949\n",
      "4500 0.5514447093009949\n",
      "4510 0.5514447093009949\n",
      "4520 1.5514447689056396\n",
      "4530 0.5514447093009949\n",
      "4540 0.5514447093009949\n",
      "4550 1.5514447689056396\n",
      "4560 1.5514447689056396\n",
      "4570 0.5514447093009949\n",
      "4580 0.5514447093009949\n",
      "4590 0.5514447093009949\n",
      "4600 1.5514447689056396\n",
      "4610 1.5514447689056396\n",
      "4620 0.5514447093009949\n",
      "4630 1.5514447689056396\n",
      "4640 0.5514447093009949\n",
      "4650 1.5514447689056396\n",
      "4660 1.5514447689056396\n",
      "4670 0.5514447093009949\n",
      "4680 1.5514447689056396\n",
      "4690 0.5514447093009949\n",
      "4700 0.5514447093009949\n",
      "4710 1.5514447689056396\n",
      "4720 0.5514447093009949\n",
      "4730 0.5514447093009949\n",
      "4740 1.5514447689056396\n",
      "4750 0.5514447093009949\n",
      "4760 0.5514447093009949\n",
      "4770 0.5514447093009949\n",
      "4780 0.5514447093009949\n",
      "4790 0.5514447093009949\n",
      "4800 0.5514447093009949\n",
      "4810 0.5514447093009949\n",
      "4820 1.5514447689056396\n",
      "4830 0.5514447093009949\n",
      "4840 0.5514447093009949\n",
      "4850 1.5514447689056396\n",
      "4860 0.5514447093009949\n",
      "4870 1.5514447689056396\n",
      "4880 0.5514447093009949\n",
      "4890 0.5514447093009949\n",
      "4900 0.5514447093009949\n",
      "4910 0.5514447093009949\n",
      "4920 0.5514447093009949\n",
      "4930 0.5514447093009949\n",
      "4940 0.5514447093009949\n",
      "4950 0.5514447093009949\n",
      "4960 1.5514447689056396\n",
      "4970 1.5514447689056396\n",
      "4980 1.5514447689056396\n",
      "4990 0.5514447093009949\n",
      "5000 0.5514447093009949\n",
      "5010 0.5514447093009949\n",
      "5020 0.5514447093009949\n",
      "5030 0.5514447093009949\n",
      "5040 0.5514447093009949\n",
      "5050 0.5514447093009949\n",
      "5060 0.5514447093009949\n",
      "5070 0.5514447093009949\n",
      "5080 1.5514447689056396\n",
      "5090 0.5514447093009949\n",
      "5100 0.5514447093009949\n",
      "5110 1.5514447689056396\n",
      "5120 1.5514447689056396\n",
      "5130 0.5514447093009949\n",
      "5140 1.5514447689056396\n",
      "5150 0.5514447093009949\n",
      "5160 1.5514447689056396\n",
      "5170 1.5514447689056396\n",
      "5180 0.5514447093009949\n",
      "5190 1.5514447689056396\n",
      "5200 0.5514447093009949\n",
      "5210 0.5514447093009949\n",
      "5220 0.5514447093009949\n",
      "5230 1.5514447689056396\n",
      "5240 0.5514447093009949\n",
      "5250 1.5514447689056396\n",
      "5260 1.5514447689056396\n",
      "5270 0.5514447093009949\n",
      "5280 0.5514447093009949\n",
      "5290 0.5514447093009949\n",
      "5300 1.5514447689056396\n",
      "5310 0.5514447093009949\n",
      "5320 0.5514447093009949\n",
      "5330 0.5514447093009949\n",
      "5340 0.5514447093009949\n",
      "5350 0.5514447093009949\n",
      "5360 1.5514447689056396\n",
      "5370 1.5514447689056396\n",
      "5380 0.5514447093009949\n",
      "5390 0.5514447093009949\n",
      "5400 0.5514447093009949\n",
      "5410 1.5514447689056396\n",
      "5420 0.5514447093009949\n",
      "5430 0.5514447093009949\n",
      "5440 0.5514447093009949\n",
      "5450 0.5514447093009949\n",
      "5460 1.5514447689056396\n",
      "5470 0.5514447093009949\n",
      "5480 0.5514447093009949\n",
      "5490 0.5514447093009949\n",
      "5500 0.5514447093009949\n",
      "5510 1.5514447689056396\n",
      "5520 1.5514447689056396\n",
      "5530 1.5514447689056396\n",
      "5540 1.5514447689056396\n",
      "5550 0.5514447093009949\n",
      "5560 1.5514447689056396\n",
      "5570 0.5514447093009949\n",
      "5580 0.5514447093009949\n",
      "5590 0.5514447093009949\n",
      "5600 0.5514447093009949\n",
      "5610 0.5514447093009949\n",
      "5620 0.5514447093009949\n",
      "5630 0.5514447093009949\n",
      "5640 0.5514447093009949\n",
      "5650 0.5514447093009949\n",
      "5660 0.5514447093009949\n",
      "5670 1.5514447689056396\n",
      "5680 0.5514447093009949\n",
      "5690 1.5514447689056396\n",
      "5700 0.5514447093009949\n",
      "5710 1.5514447689056396\n",
      "5720 0.5514447093009949\n",
      "5730 1.5514447689056396\n",
      "5740 0.5514447093009949\n",
      "5750 0.5514447093009949\n",
      "5760 0.5514447093009949\n",
      "5770 0.5514447093009949\n",
      "5780 1.5514447689056396\n",
      "5790 1.5514447689056396\n",
      "5800 1.5514447689056396\n",
      "5810 0.5514447093009949\n",
      "5820 0.5514447093009949\n",
      "5830 0.5514447093009949\n",
      "5840 1.5514447689056396\n",
      "5850 0.5514447093009949\n",
      "5860 0.5514447093009949\n",
      "5870 1.5514447689056396\n",
      "5880 0.5514447093009949\n",
      "5890 1.5514447689056396\n",
      "5900 0.5514447093009949\n",
      "5910 0.5514447093009949\n",
      "5920 0.5514447093009949\n",
      "5930 0.5514447093009949\n",
      "5940 1.5514447689056396\n",
      "5950 0.5514447093009949\n",
      "5960 0.5514447093009949\n",
      "5970 0.5514447093009949\n",
      "5980 0.5514447093009949\n",
      "5990 1.5514447689056396\n",
      "6000 0.5514447093009949\n",
      "6010 0.5514447093009949\n",
      "6020 0.5514447093009949\n",
      "6030 1.5514447689056396\n",
      "6040 1.5514447689056396\n",
      "6050 0.5514447093009949\n",
      "6060 0.5514447093009949\n",
      "6070 0.5514447093009949\n",
      "6080 0.5514447093009949\n",
      "6090 1.5514447689056396\n",
      "6100 0.5514447093009949\n",
      "6110 0.5514447093009949\n",
      "6120 0.5514447093009949\n",
      "6130 1.5514447689056396\n",
      "6140 1.5514447689056396\n",
      "6150 1.5514447689056396\n",
      "6160 1.5514447689056396\n",
      "6170 1.5514447689056396\n",
      "6180 1.5514447689056396\n",
      "6190 0.5514447093009949\n",
      "6200 0.5514447093009949\n",
      "6210 0.5514447093009949\n",
      "6220 0.5514447093009949\n",
      "6230 0.5514447093009949\n",
      "6240 0.5514447093009949\n",
      "6250 0.5514447093009949\n",
      "6260 0.5514447093009949\n",
      "6270 0.5514447093009949\n",
      "6280 0.5514447093009949\n",
      "6290 0.5514447093009949\n",
      "6300 1.5514447689056396\n",
      "6310 0.5514447093009949\n",
      "6320 0.5514447093009949\n",
      "6330 1.5514447689056396\n",
      "6340 0.5514447093009949\n",
      "6350 0.5514447093009949\n",
      "6360 1.5514447689056396\n",
      "6370 1.5514447689056396\n",
      "6380 0.5514447093009949\n",
      "6390 0.5514447093009949\n",
      "6400 1.5514447689056396\n",
      "6410 1.5514447689056396\n",
      "6420 0.5514447093009949\n",
      "6430 1.5514447689056396\n",
      "6440 0.5514447093009949\n",
      "6450 1.5514447689056396\n",
      "6460 1.5514447689056396\n",
      "6470 0.5514447093009949\n",
      "6480 0.5514447093009949\n",
      "6490 1.5514447689056396\n",
      "6500 1.5514447689056396\n",
      "6510 1.5514447689056396\n",
      "6520 0.5514447093009949\n",
      "6530 1.5514447689056396\n",
      "6540 0.5514447093009949\n",
      "6550 0.5514447093009949\n",
      "6560 1.5514447689056396\n",
      "6570 1.5514447689056396\n",
      "6580 1.5514447689056396\n",
      "6590 0.5514447093009949\n",
      "6600 1.5514447689056396\n",
      "6610 1.5514447689056396\n",
      "6620 0.5514447093009949\n",
      "6630 0.5514447093009949\n",
      "6640 0.5514447093009949\n",
      "6650 1.5514447689056396\n",
      "6660 0.5514447093009949\n",
      "6670 1.5514447689056396\n",
      "6680 1.5514447689056396\n",
      "6690 1.5514447689056396\n",
      "6700 1.5514447689056396\n",
      "6710 0.5514447093009949\n",
      "6720 1.5514447689056396\n",
      "6730 1.5514447689056396\n",
      "6740 1.5514447689056396\n",
      "6750 0.5514447093009949\n",
      "6760 0.5514447093009949\n",
      "6770 0.5514447093009949\n",
      "6780 1.5514447689056396\n",
      "6790 0.5514447093009949\n",
      "6800 1.5514447689056396\n",
      "6810 0.5514447093009949\n",
      "6820 0.5514447093009949\n",
      "6830 0.5514447093009949\n",
      "6840 0.5514447093009949\n",
      "6850 0.5514447093009949\n",
      "6860 0.5514447093009949\n",
      "6870 0.5514447093009949\n",
      "6880 0.5514447093009949\n",
      "6890 1.5514447689056396\n",
      "6900 1.5514447689056396\n",
      "6910 1.5514447689056396\n",
      "6920 0.5514447093009949\n",
      "6930 1.5514447689056396\n",
      "6940 1.5514447689056396\n",
      "6950 0.5514447093009949\n",
      "6960 0.5514447093009949\n",
      "6970 0.5514447093009949\n",
      "6980 0.5514447093009949\n",
      "6990 0.5514447093009949\n",
      "7000 0.5514447093009949\n",
      "7010 0.5514447093009949\n",
      "7020 0.5514447093009949\n",
      "7030 0.5514447093009949\n",
      "7040 1.5514447689056396\n",
      "7050 0.5514447093009949\n",
      "7060 0.5514447093009949\n",
      "7070 1.5514447689056396\n",
      "7080 0.5514447093009949\n",
      "7090 0.5514447093009949\n",
      "7100 1.5514447689056396\n",
      "7110 0.5514447093009949\n",
      "7120 1.5514447689056396\n",
      "7130 0.5514447093009949\n",
      "7140 0.5514447093009949\n",
      "7150 0.5514447093009949\n",
      "7160 1.5514447689056396\n",
      "7170 0.5514447093009949\n",
      "7180 1.5514447689056396\n",
      "7190 1.5514447689056396\n",
      "7200 1.5514447689056396\n",
      "7210 0.5514447093009949\n",
      "7220 0.5514447093009949\n",
      "7230 0.5514447093009949\n",
      "7240 0.5514447093009949\n",
      "7250 0.5514447093009949\n",
      "7260 0.5514447093009949\n",
      "7270 1.5514447689056396\n",
      "7280 0.5514447093009949\n",
      "7290 0.5514447093009949\n",
      "7300 0.5514447093009949\n",
      "7310 0.5514447093009949\n",
      "7320 0.5514447093009949\n",
      "7330 1.5514447689056396\n",
      "7340 0.5514447093009949\n",
      "7350 0.5514447093009949\n",
      "7360 0.5514447093009949\n",
      "7370 1.5514447689056396\n",
      "7380 1.5514447689056396\n",
      "7390 1.5514447689056396\n",
      "7400 1.5514447689056396\n",
      "7410 1.5514447689056396\n",
      "7420 1.5514447689056396\n",
      "7430 0.5514447093009949\n",
      "7440 1.5514447689056396\n",
      "7450 1.5514447689056396\n",
      "7460 1.5514447689056396\n",
      "7470 0.5514447093009949\n",
      "7480 1.5514447689056396\n",
      "7490 1.5514447689056396\n",
      "7500 0.5514447093009949\n",
      "7510 1.5514447689056396\n",
      "7520 0.5514447093009949\n",
      "7530 0.5514447093009949\n",
      "7540 1.5514447689056396\n",
      "7550 1.5514447689056396\n",
      "7560 0.5514447093009949\n",
      "7570 0.5514447093009949\n",
      "7580 1.5514447689056396\n",
      "7590 1.5514447689056396\n",
      "7600 0.5514447093009949\n",
      "7610 1.5514447689056396\n",
      "7620 0.5514447093009949\n",
      "7630 1.5514447689056396\n",
      "7640 0.5514447093009949\n",
      "7650 0.5514447093009949\n",
      "7660 0.5514447093009949\n",
      "7670 1.5514447689056396\n"
     ]
    }
   ],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(t.parameters(), lr=0.00001, momentum=0.9)\n",
    "loss = None\n",
    "for i in range(ds.__len__()):\n",
    "    idx = random.randint(0, ds.__len__()-1)\n",
    "    x, y = ds.__getitem__(idx)\n",
    "    y = y.long()\n",
    "    outputs = t(x)\n",
    "    loss = loss_fn(outputs, y)\n",
    "    if i%10 == 0:\n",
    "        print(i, loss.item())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "correct = 0\n",
    "true = []\n",
    "pred = []\n",
    "for i in range(1000):\n",
    "    idx = random.randint(0, ds.__len__()-1)\n",
    "    x, y = ds.__getitem__(idx)\n",
    "    y = y.long()\n",
    "    outputs = t(x)\n",
    "    outputs = torch.argmax(outputs, axis=-1)\n",
    "    true.append(y.item())\n",
    "    pred.append(outputs.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b4702013-4aec-403c-849b-04679a2ed616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score (weighted):  0.5146537530266344\n",
      "F1 Score (macro):  0.2631154156577885\n",
      "F1 Score (micro):  0.652\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "print(\"F1 Score (weighted): \", f1_score(true, pred, average=\"weighted\"))\n",
    "print(\"F1 Score (macro): \", f1_score(true, pred, average=\"macro\"))\n",
    "print(\"F1 Score (micro): \", f1_score(true, pred, average=\"micro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b2f9b2-5623-4472-a5a0-8117ab0d13d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
